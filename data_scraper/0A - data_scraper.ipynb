{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0A. Scraping Top 50 Trending YouTube videos\n",
    "\n",
    "The following is a slight modification of the [data scraper](https://github.com/DataSnaek/Trending-YouTube-Scraper) prepared by DataSnaek.\n",
    "\n",
    "The scraper has been edited to only scrape the top trending videos of the day of only US and GB (UK)\n",
    "- Only the top trending videos of the current day are retrievable from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Import Libraries](#import_libraries)\n",
    "2. [Read in API key](#read_api_key)\n",
    "3. [Initialise Variables](#initialise_variables)\n",
    "    1. [Initialise Features and Headers of .csv Files](#initialise_features_and_headers)\n",
    "    2. [Initialise List of Country Codes and Output Directory](#initialise_cc_and_output_dir)\n",
    "4. [Scraper Functions](#scraper_functions)\n",
    "5. [Write Data to .csv Files](#write_data_to_.csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries <a name=\"import_libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, sys, time, os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in API key <a name=\"read_api_key\"></a>\n",
    "\n",
    "You will need to obtain a YouTube Data API key. Instructions for obtaining one can be found [here](https://developers.google.com/youtube/registering_an_application).\n",
    "\n",
    "Place the file into a text file named *api_key.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt', 'r') as f:\n",
    "    api_key = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Variables <a name=\"initialise_variables\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise Features and Headers of .csv Files <a name=\"initialise_features_and_headers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of simple to collect features\n",
    "snippet_features = [\"title\", \"publishedAt\", \"channelId\", \"channelTitle\", \"categoryId\"]\n",
    "\n",
    "# Any characters to exclude, generally these are things that become problematic in CSV files\n",
    "unsafe_characters = ['\\n', '\"']\n",
    "\n",
    "# Used to identify columns, currently hardcoded order\n",
    "header = [\"video_id\"] + snippet_features + [\"trending_date\", \"tags\", \"view_count\", \"likes\", \"dislikes\",\n",
    "                                            \"comment_count\", \"thumbnail_link\", \"comments_disabled\",\n",
    "                                            \"ratings_disabled\", \"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise List of Country Codes and Output Directory <a name=\"initialise_cc_and_output_dir\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = ['US', 'GB']\n",
    "output_dir = Path('.')/'scraper-output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper Functions <a name=\"scraper_functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature(feature):\n",
    "    # Removes any character from the unsafe characters list and surrounds the whole item in quotes\n",
    "    for ch in unsafe_characters:\n",
    "        feature = str(feature).replace(ch, \"\")\n",
    "    return f'\"{feature}\"'\n",
    "\n",
    "\n",
    "def api_request(page_token, country_code):\n",
    "    # Builds the URL and requests the JSON from it\n",
    "    request_url = f\"https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet{page_token}chart=mostPopular&regionCode={country_code}&maxResults=50&key={api_key}\"\n",
    "    request = requests.get(request_url)\n",
    "    if request.status_code == 429:\n",
    "        print(\"Timed out: Too many requests, please try again later.\\n\")\n",
    "        sys.exit()\n",
    "    return request.json()\n",
    "\n",
    "\n",
    "def get_tags(tags_list):\n",
    "    # Takes a list of tags, prepares each tag and joins them into a string by the pipe character\n",
    "    return prepare_feature(\"|\".join(tags_list))\n",
    "\n",
    "\n",
    "def get_videos(items):\n",
    "    lines = []\n",
    "    for video in items:\n",
    "        comments_disabled = False\n",
    "        ratings_disabled = False\n",
    "\n",
    "        # We can assume something is wrong with the video if it has no statistics, often this means it has been deleted\n",
    "        # so we can just skip it\n",
    "        if \"statistics\" not in video:\n",
    "            continue\n",
    "\n",
    "        # A full explanation of all of these features can be in the original GitHub project link\n",
    "        video_id = prepare_feature(video['id'])\n",
    "\n",
    "        # Snippet and statistics are sub-dicts of video, containing the most useful info\n",
    "        snippet = video['snippet']\n",
    "        statistics = video['statistics']\n",
    "\n",
    "        # This list contains all of the features in snippet that are 1 deep and require no special processing\n",
    "        features = [prepare_feature(snippet.get(feature, \"\")) for feature in snippet_features]\n",
    "\n",
    "        # The following are special case features which require unique processing, or are not within the snippet dict\n",
    "        description = snippet.get(\"description\", \"\")\n",
    "        thumbnail_link = snippet.get(\"thumbnails\", dict()).get(\"default\", dict()).get(\"url\", \"\")\n",
    "        trending_date = time.strftime(\"%y.%d.%m\")\n",
    "        tags = get_tags(snippet.get(\"tags\", [\"[none]\"]))\n",
    "        view_count = statistics.get(\"viewCount\", 0)\n",
    "\n",
    "        # This may be unclear, essentially the way the API works is that if a video has comments or ratings disabled\n",
    "        # then it has no feature for it, thus if they don't exist in the statistics dict we know they are disabled\n",
    "        if 'likeCount' in statistics and 'dislikeCount' in statistics:\n",
    "            likes = statistics['likeCount']\n",
    "            dislikes = statistics['dislikeCount']\n",
    "        else:\n",
    "            ratings_disabled = True\n",
    "            likes = 0\n",
    "            dislikes = 0\n",
    "\n",
    "        if 'commentCount' in statistics:\n",
    "            comment_count = statistics['commentCount']\n",
    "        else:\n",
    "            comments_disabled = True\n",
    "            comment_count = 0\n",
    "\n",
    "        # Compiles all of the various bits of info into one consistently formatted line\n",
    "        line = [video_id] + features + [prepare_feature(x) for x in [trending_date, tags, view_count, likes, dislikes,\n",
    "                                                                     comment_count, thumbnail_link, comments_disabled,\n",
    "                                                                     ratings_disabled, description]]\n",
    "        lines.append(\",\".join(line))\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_pages(country_code, next_page_token=\"&\"):\n",
    "    country_data = []\n",
    "\n",
    "    # Because the API uses page tokens (which are literally just the same function of numbers everywhere) it is much\n",
    "    # more inconvenient to iterate over pages, but that is what is done here.\n",
    "    while next_page_token is not None:\n",
    "        # A page of data i.e. a list of videos and all needed data\n",
    "        video_data_page = api_request(next_page_token, country_code)\n",
    "\n",
    "        # Get the next page token and build a string which can be injected into the request with it, unless it's None,\n",
    "        # then let the whole thing be None so that the loop ends after this cycle\n",
    "        next_page_token = video_data_page.get(\"nextPageToken\", None)\n",
    "        next_page_token = f\"&pageToken={next_page_token}&\" if next_page_token is not None else next_page_token\n",
    "\n",
    "        # Get all of the items as a list and let get_videos return the needed features\n",
    "        items = video_data_page.get('items', [])\n",
    "        country_data += get_videos(items)\n",
    "\n",
    "    return country_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(country_code, country_data):\n",
    "    print(f\"Writing {country_code} data for {time.strftime('%y.%d.%m')} to file...\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(f\"{output_dir}/{time.strftime('%y.%d.%m')}_{country_code}_videos.csv\", \"w+\", encoding='utf-8') as file:\n",
    "        for row in country_data:\n",
    "            file.write(f\"{row}\\n\")\n",
    "\n",
    "            \n",
    "# Driver function\n",
    "def get_data():\n",
    "    for country_code in country_codes:\n",
    "        country_data = [\",\".join(header)] + get_pages(country_code)\n",
    "        write_to_file(country_code, country_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data to .csv Files <a name=\"write_data_to_.csv_files\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing US data for 19.10.11 to file...\n",
      "Writing GB data for 19.10.11 to file...\n"
     ]
    }
   ],
   "source": [
    "get_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
